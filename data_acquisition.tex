In research departments consisting of several experimental setups decentralized
data acquisition from independent clients to a central server is a very
attractive solution. This is due to the large flexibility of the system, low
probability of dataloss and to simplify backup procedures of all experimental
data. To be able to save acquired data from a range of different computer
hardware and scientific instruments, interfacing of many types of equipment is
necessary.

In practice this is achieved by formulating a set of general design goals that
will serve as computer language and hardware independent pseudo-code which will
help the process of designing a new acquisition client to obtain conformity of
the system. The design goals of our implementation are:

* To minimize data loss and to provide live data access, 
  all clients must store data for as short amount of time as
  possible before handing of the data to a central server. Preferably, the data
  should be stored on the server as soon as it is acquired. For data being
  recorded over longer time-spans this means that data must be live-streamed to
  the server.

* To avoid data-loss in the event of network failure or maintenance
  of the central server, all clients collecting critical data must implement a
  local queue that will temporarily hold data until the central server can
  again be accessed. The client must continuously check if the server is
  available again and as soon as possible deliver queued data to the server.

* For continuous measurements (e.g. temperatures, pressure in vacuum chambers,
  cooling water flow, etc.) data logging must be implemented in a way that
  ensures that all significant events are recorded and at the same time does
  not use excessive amounts of storage. This is typically implemented by
  sampling data with a much higher rate than they are recorded. The local
  client will then, based on relevant heuristics, decide whether a new data
  point should stored on the server. In practise this is often implemented by
  waiting for either a given relative change in the signal or a predefined time
  since last recording of a data point. An example of such an implementation
  in python is shown below.

\begin{verbatim}
max_time_between_points = 600; deviation = 0.1
now = time.time(); current_measurement = get_measurement()
time_trigged = (now - last_recorded_time) > max_time_between_points
value_trigged = not (last_recorded_value * (1-deviation)
                     < current_measurement <
                     last_recorded_value * (1+deviation))
if time_trigged or value_trigged:
    last_recorded_value = current_measurement
    last_recorded_time = now
    send_measurement_to_db(now, current_measurement)
\end{verbatim}
  
* For stand-alone offline measurements (e.g. spectrometry, electrical
  characterization, etc.) it is important to measure as much metadata as
  possible to ensure that all information connected to the experiment is
  preserved. Along with accurate time information this will ensure that
  questions that where not yet formulated at the time of the experiment can in
  some cases be answered in retrospect using the metadata along with the
  continuously measured data.
  
